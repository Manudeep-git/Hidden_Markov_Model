{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "markov_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52Ql8WzMULf_"
      },
      "source": [
        "# Word generation and prediction using Hidden Markov Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSXPdKtlUYRr"
      },
      "source": [
        "The aim of this workbook is to design a algorithm similar to hidden markov model to learn correlations and distributions to perform\n",
        "  1. Generate new text from given text corpus dataset\n",
        "  2. Perform text prediction from given sequence of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQoK8tvKVMUV"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ5hUlJFEeHf"
      },
      "source": [
        "import string \n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0r0Q_G9Va-2"
      },
      "source": [
        "## Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8udkzh71VX-Q"
      },
      "source": [
        "data = \"/content/alllines.txt\" ##replace content with data when executing "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix9JAfKQVwzl"
      },
      "source": [
        "## Pre-processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5DGGkiaWCMZ"
      },
      "source": [
        "Let's remove some special characters in each line of text for better results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkEwP_TBVtnl"
      },
      "source": [
        "def removeSpecial(line):\n",
        "  return line.translate(str.maketrans(\"\",\"\",string.punctuation))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Ra1nCIZ22s"
      },
      "source": [
        "Creating a hashmap/dictionary for storing key-value pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnW4OYAPYlfa"
      },
      "source": [
        "def create_dict(dictionary, key, value):\n",
        "  if key not in dictionary:\n",
        "      dictionary[key]=[]\n",
        "  dictionary[key].append(value)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Pa74zfZyav"
      },
      "source": [
        "Creating a probability dict\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qV6Km1OZHEk"
      },
      "source": [
        "def create_probability_dict(text_data):\n",
        "    prob_dict = {}\n",
        "    text_data_len = len(text_data)\n",
        "    for item in text_data:\n",
        "        prob_dict[item] = prob_dict.get(item, 0) + 1\n",
        "    for key, value in prob_dict.items():\n",
        "        prob_dict[key] = value / text_data_len\n",
        "    return prob_dict"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO3pYEfKZ7IT"
      },
      "source": [
        "Now we need some data-structure to hold initial states and trnasition states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoMYT2aVaETB"
      },
      "source": [
        "initial_word= {}\n",
        "second_word = {}\n",
        "transitions = {}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGaZqSQCbuJB"
      },
      "source": [
        "## Building and Training the Markov Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHI0aCWrb_f8"
      },
      "source": [
        "One important property about markov model is that the **next step** depends on only the **current step** and not on the past historical steps\n",
        "\n",
        "For this project, I'm gonna make use of the same "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdiaR4uecgpL"
      },
      "source": [
        "The training of the Markov model can be divided into the following stages -\n",
        "1. Cleaning  up data and Tokenisation\n",
        "2. Building the state pairs(previous and current)\n",
        "3. Determining the probability distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya9ezNsDb4bI"
      },
      "source": [
        "# Trains a Markov model based on the data in data_file\n",
        "def build_and_train_markov_model():\n",
        "    for line in open(data):\n",
        "\n",
        "        #tokenizing data\n",
        "        tokens = removeSpecial(line.rstrip().lower()).split()\n",
        "        tokens_length = len(tokens)\n",
        "\n",
        "        #next and current state-pairs\n",
        "        for i in range(tokens_length):\n",
        "            token = tokens[i]\n",
        "\n",
        "            #Initial state need not be calculated for 1st token\n",
        "            if i == 0:\n",
        "                initial_word[token] = initial_word.get(token, 0) + 1\n",
        "            else:\n",
        "                prev_token = tokens[i - 1]\n",
        "\n",
        "                ##additional token for last-item\n",
        "                if i == tokens_length - 1:\n",
        "                    create_dict(transitions, (prev_token, token), 'END')\n",
        "                if i == 1:\n",
        "                    create_dict(second_word, prev_token, token)\n",
        "                else:\n",
        "                    prev_prev_token = tokens[i - 2]\n",
        "                    create_dict(transitions, (prev_prev_token, prev_token), token)\n",
        "    \n",
        "    # Normalize the distributions\n",
        "    initial_word_total = sum(initial_word.values())\n",
        "    for key, value in initial_word.items():\n",
        "        initial_word[key] = value / initial_word_total\n",
        "        \n",
        "    for prev_word, next_word_list in second_word.items():\n",
        "        second_word[prev_word] = create_probability_dict(next_word_list)\n",
        "        \n",
        "    for word_pair, next_word_list in transitions.items():\n",
        "        transitions[word_pair] = create_probability_dict(next_word_list)\n",
        "    \n",
        "    print('Building and Training finished')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQGUvxpGd5tE",
        "outputId": "b09f46eb-d1e6-45bb-c718-5f7f457696d0"
      },
      "source": [
        "build_and_train_markov_model()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building and Training finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y0f_OEIeKWj"
      },
      "source": [
        "## 1.Generating new text from corpus using **Built Hidden Markov Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ-jZHrEedXG"
      },
      "source": [
        "Once we have completed the training, we will have the initial word distribution, second-word distribution and the state transition distributions. Next to generate a text corpus all we need is to write a function to sample out from the above-created distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBp4Zl4teeHp"
      },
      "source": [
        "def sample_word(dictionary):\n",
        "    p0 = np.random.random()\n",
        "    cumulative = 0\n",
        "    for key, value in dictionary.items():\n",
        "        cumulative += value\n",
        "        if p0 < cumulative:\n",
        "            return key\n",
        "    assert(False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aExeYtuAeh8H"
      },
      "source": [
        "#Fixing our generated text to length 15\n",
        "number_of_sentences = 12"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gwKRYQGenLL"
      },
      "source": [
        "def generate_text():\n",
        "    for i in range(number_of_sentences):\n",
        "        sentence = []\n",
        "        # Initial word\n",
        "        word0 = sample_word(initial_word)\n",
        "        sentence.append(word0)\n",
        "        # Second word\n",
        "        word1 = sample_word(second_word[word0])\n",
        "        sentence.append(word1)\n",
        "        # Subsequent words untill END\n",
        "        while True:\n",
        "            word2 = sample_word(transitions[(word0, word1)])\n",
        "            if word2 == 'END':\n",
        "                break\n",
        "            sentence.append(word2)\n",
        "            word0 = word1\n",
        "            word1 = word2\n",
        "        print(' '.join(sentence))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu1lntFyen_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26150c1-1992-4254-8230-7ead385d6f4d"
      },
      "source": [
        "generate_text()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "thou art a villain\n",
            "scene i london an antechamber in the mercy of the state some service and they have bought the cottage pasture and the complexion of a pure blush thou mayst\n",
            "what you say shes dead\n",
            "scene iv rome philarios house\n",
            "the bell\n",
            "or what you say well but thou didst abuse\n",
            "you promised knighthood to our presence\n",
            "traitors away he rests not in hate\n",
            "tis better using france than trusting france\n",
            "o serpent heart hid with a proclamation that you asked her\n",
            "what chance is this\n",
            "under your sentence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR7X40nseuqI"
      },
      "source": [
        "## 2.Performing text prediction given a sequence of words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqEgFBCeexGB"
      },
      "source": [
        "def text_prediction(text):\n",
        "        text = removeSpecial(text.lower()).split()\n",
        "        # Initial word\n",
        "        word0 = text[0]\n",
        "        # Second word\n",
        "        if len(text) == 1:\n",
        "            word1 = sample_word(second_word[word0])\n",
        "            text.append(word1)\n",
        "        else:\n",
        "            word1 = text[1]\n",
        "        # Subsequent words untill END\n",
        "        while True:\n",
        "            word2 = max(transitions[(word0, word1)], key=transitions[(word0, word1)].get)\n",
        "            if word2 == 'END':\n",
        "                break\n",
        "            text.append(word2)\n",
        "            word0 = word1\n",
        "            word1 = word2\n",
        "        print(' '.join(text))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz59Qi-ze4YS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55238ce1-6dbe-41fc-82ee-a6dfeadaf61c"
      },
      "source": [
        "#Testing\n",
        "text_prediction(\"Whose arms\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whose arms were moulded in their own\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKSWUFJGe9F-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2cd9ef-222f-415a-d4a8-24a3fdc6d7cb"
      },
      "source": [
        "text_prediction(\"Of hostile\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "of hostile paces those opposed eyes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}